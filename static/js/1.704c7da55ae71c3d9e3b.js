webpackJsonp([1,182],{432:function(n,e){n.exports={rawContent:"\n我被这个[一群鱼](http://synaptic.juancazala.com/#/)迷住了\n\n意译自[https://github.com/cazala/synaptic/wiki/Neural-Networks-101](https://github.com/cazala/synaptic/wiki/Neural-Networks-101)\n\n一个不怎么关乎公式的简单神经网络介绍。\n\n## 神经元(Neuron)\n\n神经网络的基本单位。本质上，神经元有树突(输入)、细胞体(处理器)和轴突(输出)。\n\n自然界中，激活过程大概这样：当神经元的累积加权输入超过特定阈值，轴突激发信号。\n\n神经元最重要的特性是： 学习。\n\n人工神经元是这样的：\n\n![人工神经元](https://camo.githubusercontent.com/8b87e593fb9382c16a81cc059d994adec259a1c4/687474703a2f2f692e696d6775722e636f6d2f643654374b39332e706e67)\n\n它有几个输入，每个输入对应有权重(特定连接的重要性)。当要激活神经元的时候，  \n通过累加加权输入计算它的状态。但是神经元总有一个为1的额外的输入，叫偏差(bias)。\n\n这确保既是所有输入都是0， 神经元仍然有输入。\n\n![](https://camo.githubusercontent.com/875a8f9cef6889b90dcd82752d12569456f385f0/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f735f6a25323025334425323025354373756d5f25374269253744253230775f253742696a2537442e795f25374269253744)\n\n计算神经元状态(state)之后，神经元将值传递给激活函数(activation function)。  \n该函数将结果正则化(normalize)(到0-1)\n\n![](https://camo.githubusercontent.com/198be5a2e3b4b1e57758d46e4c324ac2c22671bd/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f795f6a253230253344253230665f6a253238535f6a253239)\n\n### 激活函数\n\n激活函数通常是sigmoid函数，不是[Logistic](https://en.wikipedia.org/wiki/Logistic_function)()就是[Hyperbolic Tangent](http://mathworld.wolfram.com/HyperbolicTangent.html)(双曲正切).\n\n![logistic](https://camo.githubusercontent.com/0bd79e6fd612e898fda2e04caa797648b8c0bed5/687474703a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f382f38382f4c6f6769737469632d63757276652e7376672f33323070782d4c6f6769737469632d63757276652e7376672e706e67)\n\n## 前向传播网络(Feed-forward Network)\n\n这是最简单的架构，神经元保存在层中，上一层的神经元连接所有下一层的神经元，  \n每一层的输出又是下一层的输入。\n\n![feed-forward network](https://camo.githubusercontent.com/0699ff876dbb371e894dec939c719287f5729aa9/687474703a2f2f692e696d6775722e636f6d2f3375394f52616c2e6a70673f31)\n\n第一层(输入层)从环境接受输入，激活，它的输出作为下一层的输入，直到抵达最终层(输出层)。\n\n### 神经网络如何学习\n\n通过训练。用来做这个算法叫做反向传播(backpropagation)。在给定网络输入后，  \n将产生输出。\n\n接着，告诉网络该输入的理想输出。\n\n下次，网络将采取这个理想输出并调整权重来产生更精确的输出，\n\n从输出层反向调整直到输入层。\n\n那么，下一次接受同样输入输出层将输出更接近的值，这个过程迭代多次，\n\n直到网络输出和理想输出差距足够小。\n\n### 但是，反向传播如何工作？\n\n算法通过[梯度下降](http://en.wikipedia.org/wiki/Gradient_descent)(Gradient Descent)来计算调整权重\n\n比如如下图表示一个特定权重和误差(网络输出和理想输出差距)的关系。\n\n![error vs weight](https://camo.githubusercontent.com/e6a0e02bd080acc585a622d2c03ca6e44a9e9adc/687474703a2f2f692e696d6775722e636f6d2f36565a6542706e2e706e67)\n\n该算法计算实际权重位置的梯度，也叫瞬时斜率(instant slope)(图中箭头)\n\n它将向减小误差的方向移动，\n\n这个过程将对网络中每个权重重复。\n\n![](https://camo.githubusercontent.com/eb2584182e5a40170553e668aa52cebb8a28c486/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f736c6f7065253230253344253230253543667261632537422535437061727469616c253230452537442537422535437061727469616c253230775f695f6a253744)\n\n为了计算梯度和调整权重，我们使用增量(δ)法则\n\n### 增量法则\n\n输出层θ的增量使用注入误差(injected error)(网络输出和理想输出的差距)  \n来计算。\n\n![](https://camo.githubusercontent.com/4710fab5a0e844de6d1f1724c1b88b8c787baf08/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f455f2535435468657461253230253344253230742532302d253230795f2535435468657461)  \n![](https://camo.githubusercontent.com/3b840b789a63f6b24b0bfa7fb988661af225ffc7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354364656c74612532305f2535435468657461253230253344253230455f25354354686574612532302e66253237253238735f2535435468657461253230253239)\n\nf’是激活函数的导函数。’\n\n这个误差反向传播到输入层，每一层都使用上一层的δs来计算本层的δ\n\n![](https://camo.githubusercontent.com/52802a14da3a104ba1c5a615c4549723a9d8857f/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f455f6a25323025334425323025354373756d25323025354364656c74612532305f6b253230775f6b5f6a)  \n![](https://camo.githubusercontent.com/cb17a45676307f2e0d93da911284ce4a9d99a172/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354364656c74615f6a253230253344253230455f6a2e66253237253238535f6a253239)\n\n我们使用delta来计算每个权重的梯度：\n\n![](https://camo.githubusercontent.com/8cc240df2a9475c2a0b1efd25fc20da49c9d10a4/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f253543667261632537422535437061727469616c253230455f6a2537442537422535437061727469616c253230775f695f6a25374425323025334425323025354364656c74615f6a2532302e253230795f69)\n\n现在根据反向传播更新权重：\n\n![](https://camo.githubusercontent.com/052920e4d54e7c529bc571ef6c7c9a38f5c0e74a/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354344656c7461253230775f695f6a253230253344253230253543766172657073696c6f6e253230253543667261632537422535437061727469616c253230455f6a2537442537422535437061727469616c253230775f695f6a253744)\n\n这里ε是学习率(learning rate).\n\n译注: 推导见[维基百科](https://en.wikipedia.org/wiki/Backpropagation#Derivation)\n\n## 时间递归神经网络(Recurrent Neural Networks)\n\n这个网络中的神经元自连接(固定权重1)，这让它们有某种短期记忆。\n\n![](https://camo.githubusercontent.com/a60c701dea057ea0ac5634b09713b3d3e4867ec8/687474703a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f642f64642f526563757272656e744c617965724e657572616c4e6574776f726b2e706e67)\n\n过去激励的额外输入使网络有某种上下文信息，有助于特定任务产生更好的输出。\n\n在序列预测任务上，这种神经网络非常有效，但是它们不能记得过去太多步的相关信息\n\n### Constant Error Carousel\n\nCEC包含自连接的神经元，我们称为记忆细胞，一个线性激励函数(linear activation)。\n\n这使误差保持更长时间，修复了时间递归神经网络的梯度退化问题–RNN缩放每个激励的误差  \n因为squashing function的导数，当它在时空上往回传播,误差会指数消失或分叉(diverge，就是震荡)。听起来真他妈的酷!\n\n译注：问题在于误差会因为和squashing function(比如logistic激励函数)的导数相乘，还和自连接权重相乘(所以规定为1)，  \n如果squashing function是线性的，那么其导数也为1，则，误差就不会消失或分叉。\n\n### Gates\n\n有的结构不仅将神经元连接，而且调控流过这些连接的信息，这些结构叫做二阶神经网络(second order neural networks)\n\n译注：因为gate是和连接相乘，则成为二阶了。\n\n一种保护记忆细胞远离噪音输入和注入误差的方式是使用gates来缩放(scale，调控意)记忆细胞和输入输出层之间的连接。\n\n![](https://camo.githubusercontent.com/62372f7a1977651a77ffe961f1d23fb86b5472af/687474703a2f2f7777772e77696c6c616d657474652e6564752f253745676f72722f636c61737365732f63733434392f666967732f6c73746d2e676966)\n\n这就是[Long Short-Term Memory](http://en.wikipedia.org/wiki/Long_short_term_memory)的起源。LSTM是一种适合分类、处理和  \n预测时间序列的结构，特别是当关键事件中有许多非常长且未知的时间延迟。\n\n自从它的概念被第三个gate提升，叫做Forget Gate，这个管理记忆细胞的自连接，决定有多少  \n误差应该被记住，并且何时忘记。通过在每个time-step后scaling来自细胞状态的反馈实现。  \n这保护了状态不至于分叉(diverging, 同上)和崩塌(collapsing，消失)。\n\nLSTM通过来自记忆细胞到所有它的其他gates的窥视孔(peephole)连接，改善它们的性能。  \n因为它们有有关它们保护细胞的信息。实际的LSTM结构看起来像这样。\n\n![](https://camo.githubusercontent.com/90fd23066ab0110cd317bfb580a13544f46b9ec9/687474703a2f2f692e696d6775722e636f6d2f4a7046363577632e706e67)\n\n时间递归和二阶神经网络(Recurrent and Second Order Neural Networks)比前向传播神经网络相比复杂。数学细节可以参考[Derek Monner的论文](http://www.overcomplete.net/papers/nn2012.pdf)\n\n译者注：关于LSTM，[colah的文章](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)讲的很清晰易懂。\n\n有了这些基础知识，现在你可以好好玩神经网络了。\n",metaData:{layout:"post",title:"神经网络101",excerpt:"意译自https://github.com/cazala/synaptic/wiki/Neural-Networks-101, 一个不怎么关乎公式的简单神经网络介绍。",category:"machine-learning",tags:["javascript","neural-network"],disqus:!0}}}});