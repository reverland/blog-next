{"version":3,"sources":["webpack:///static/js/49.95dd52c9bf4904e9f0a2.js","webpack:///./src/posts/2014-06-02-.md"],"names":["webpackJsonp","297","module","exports"],"mappings":"AAAAA,cAAc,GAAG,MAEXC,IACA,SAASC,EAAQC,GCHvBD,EAAAC,QAAA","file":"static/js/49.95dd52c9bf4904e9f0a2.js","sourcesContent":["webpackJsonp([49,170],{\n\n/***/ 297:\n/***/ function(module, exports) {\n\n\tmodule.exports = \"\\n\\n# 为何实现一个BP神经网络？\\n\\n> \\\"What I cannot create, I do not understand\\\"\\n> \\n> --- Richard Feynman,  February 1988\\n\\n---\\n\\n## 实现一个BP神经网络的7个步骤\\n\\n1. 选择神经网络 *结构*\\n2. *随机* 初始化权重\\n3. 实现 *前向传播* \\n4. 实现 *成本函数* $J(\\\\Theta)$\\n5. 实现反向传播算法并计算 *偏微分* $\\\\frac{\\\\partial}{\\\\partial\\n\\\\Theta_{jk}^{(i)}}J(\\\\Theta)$\\n6. 使用 *梯度检查* 并在检查后关闭\\n7. 使用梯度下降或其它优化算法和反向传播来 *优化* $\\\\Theta$ 的函数 $J(\\\\Theta)$\\n\\n---\\n\\n## 数据集\\n\\n我们选取著名的鸢尾花数据集作为神经网络作用的对象，首先先观察下数据。\\n\\n```python\\nIn[2]:\\nimport numpy as np\\nfrom sklearn import datasets\\niris = datasets.load_iris()\\n# random it\\nperm = np.random.permutation(iris.target.size)\\niris.data = iris.data[perm]\\niris.target = iris.target[perm]\\nprint iris.data.shape\\nprint iris.target.shape \\nnp.unique(iris.target)\\n#print iris\\nOut[2]:\\n    (150, 4)\\n    (150,)\\n\\n    array([0, 1, 2])\\n```\\n\\n可见，鸢尾花数据集有四个特征，分为三种类别，总共150个条数据。\\n\\n---\\n\\n# 选择人工神经网络结构\\n\\n![神经网络架构图](/images/nn.png)\\n\\n---\\n\\n# 随机初始化权重\\n\\n## 网络权重随机初始化\\n\\n 初始化 $\\\\Theta_{ij}^{(l)}$ 为  $[-\\\\epsilon, \\\\epsilon]$ 之间的随机值。\\n\\n 初始化函数可定义如下：\\n\\n```python\\nIn[3]:\\ndef random_init(input_layer_size, hidden_layer_size, classes, init_epsilon=0.12):\\n    Theta_1 = np.random.rand(hidden_layer_size, input_layer_size + 1) * 2 * init_epsilon -init_epsilon\\n    Theta_2 = np.random.rand(classes, hidden_layer_size + 1) * 2 * init_epsilon -init_epsilon\\n    return (Theta_1, Theta_2)\\n```\\n\\n---\\n\\n# 实现前向传播\\n\\n每一层都以此为公式计算下一层：\\n\\n$$\\nG_{\\\\theta^{(l)}(X)} = \\\\frac{1}{1 + e^{-X(\\\\Theta^{(l)})^T}}\\n$$\\n\\n注意添加偏差项。\\n\\n---\\n\\n### 前向传播函数定义如下\\n\\n```python\\nIn[4]:\\n# 先定义一个sigmoid函数\\ndef g(theta, X):\\n    return 1 / (1 + np.e ** (-np.dot(X, np.transpose(theta))))\\n# 前向传播函数\\ndef predict(Theta_1, Theta_2, X):\\n    h = g(Theta_1, np.hstack((np.ones((X.shape[0], 1)), X)))\\n    o = g(Theta_2, np.hstack((np.ones((h.shape[0], 1)), h)))\\n    return o\\n```\\n\\n---\\n\\n# 实现成本函数\\n\\n## 成本函数的数学表示\\n\\n$$\\nJ(\\\\Theta) = -\\\\frac{1}{m}[\\\\sum_{i=1}^m\\n\\\\sum_{k=1}^Ky_k^{(i)}\\\\log(h_\\\\Theta(x^{(i)}))_k + (1 -\\ny_k^{(i)})\\\\log(1-h_\\\\Theta(x^{(i)}))_k)] + \\\\frac{\\\\lambda}{2m}\\\\sum_{l=1}^{L-1}\\\\sum\\n_{i=1}^{s_l}\\\\sum_{j=1}^{s_{l+1}}(\\\\Theta_{ji}^{(l)})^2\\n$$\\n\\n$K$是输出层单元个数, $L$是层数，我们这里是三， $s_l$ 是层 $l$ 中单元数。\\n\\n---\\n\\n```python\\nIn[5]:\\ndef costfunction(nn_param, input_layer_size, hidden_layer_size, classes, X, y, lamb_da):\\n    Theta_1 = nn_param[:hidden_layer_size * (input_layer_size + 1)].reshape((hidden_layer_size, (input_layer_size + 1)))\\n    Theta_2 = nn_param[hidden_layer_size * (input_layer_size + 1):].reshape((classes, (hidden_layer_size + 1)))\\n    m = X.shape[0]\\n    # recode y\\n    y_recoded = np.eye(classes)[y,:]\\n\\n    # print y_recoded\\n    # calculate regularator\\n    regularator = (np.sum(Theta_1**2) + np.sum(Theta_2 ** 2)) * (lamb_da/(2*m))\\n\\n    a3 = predict(Theta_1, Theta_2, X)\\n\\n    J = 1.0 / m * np.sum(-1 * y_recoded * np.log(a3)-(1-y_recoded) * np.log(1-a3)) + regularator\\n    # print J\\n    return J\\n```\\n\\n因为scipy中优化函数要求输入参数是向量而不是矩阵，因此我们的函数实现也必须能够随时展开和复原矩阵。\\n\\n---\\n\\n# 实现反向传播\\n\\n* 正向传播计算网络输出\\n* 反向计算每一层的误差$\\\\sigma^{(l)}$\\n* 由误差累加计算得到成本函数的偏微分$\\\\frac{\\\\partial}{\\\\partial\\\\Theta_{ij}^{(l)}}J(\\\\Theta)$\\n\\n同样，在实现函数时因为要传递给`scipy.optimize`模块中的优化函数，必须能展开矩阵参数为向量并可随时复原。\\n\\n详细实现过程参见Andrew NG在Coursera上的讲座。\\n\\n数学推导过程请 *自行* 上coursera ml课程论坛搜索。\\n\\n---\\n\\n```python\\nIn[6]:\\ndef Gradient(nn_param, input_layer_size, hidden_layer_size, classes, X, y, lamb_da):\\n    \\n    Theta_1 = nn_param[:hidden_layer_size * (input_layer_size + 1)].reshape((hidden_layer_size, (input_layer_size + 1)))\\n    Theta_2 = nn_param[hidden_layer_size * (input_layer_size + 1):].reshape((classes, (hidden_layer_size + 1)))\\n    \\n    m = X.shape[0]\\n    \\n    Theta1_grad = np.zeros(Theta_1.shape);\\n    Theta2_grad = np.zeros(Theta_2.shape);\\n\\n    for t in range(m):\\n        # For the input layer, where l=1:\\n        # add a bias unit and forward propagate\\n        a1 = np.hstack((np.ones((X[t:t+1,:].shape[0], 1)), X[t:t+1,:]))\\n\\n        # For the hidden layers, where l=2:\\n        a2 = g(Theta_1, a1)\\n        a2 = np.hstack((np.ones((a2.shape[0], 1)), a2))\\n        a3 = g(Theta_2, a2)\\n\\n        yy = np.zeros((1, classes))\\n        yy[0][y[t]] = 1\\n        # For the delta values:\\n        delta_3 = a3 - yy;\\n```\\n\\n---\\n\\n续上页\\n\\n```python\\n        delta_2 = delta_3.dot(Theta_2) * (a2 * (1 - a2))\\n        delta_2 = delta_2[0][1:] # Taking of the bias row\\n        delta_2 = np.transpose(delta_2)\\n        delta_2 = delta_2.reshape(1, (np.size(delta_2)))\\n    \\n        # delta_1 is not calculated because we do not associate error with the input\\n\\n        # Big delta update\\n        Theta1_grad = Theta1_grad + np.transpose(delta_2).dot(a1);\\n        Theta2_grad = Theta2_grad + np.transpose(delta_3).dot(a2);\\n    \\n    Theta1_grad = 1.0 / m * Theta1_grad + lamb_da / m * np.hstack((np.zeros((Theta_1.shape[0], 1)), Theta_1[:, 1:]))\\n    Theta2_grad = 1.0 / m * Theta2_grad + lamb_da / m * np.hstack((np.zeros((Theta_2.shape[0], 1)), Theta_2[:, 1:]))\\n\\n    Grad = np.concatenate((Theta1_grad.ravel(), Theta2_grad.ravel()), axis=0)\\n    return Grad\\n```\\n\\n---\\n\\n# 梯度检测\\n\\n> \\\"But back prop as an algorithm has a lot of details and,you know, can be a\\nlittle bit tricky to implement.\\\"\\n> \\n> -- Andrew NG如是说\\n\\n- 即使看上去成本函数在一直减小，可能神经网络还是有问题。\\n- 梯度检测能让你确认，哦，我的实现还正常。\\n\\n---\\n\\n## 梯度检测原理\\n\\n简单的微分近似。但相比反向传播神经网络算法计算量更大。\\n\\n$$\\n\\\\frac{\\\\partial}{\\\\partial\\\\theta_{i}}J(\\\\theta) \\\\approx\\n\\\\frac{J(\\\\theta_{i}+\\\\epsilon) - J(\\\\theta_{i}+\\\\epsilon) }{2\\\\epsilon}\\n$$\\n\\n### 这种梯度计算实现如下：\\n\\n```python\\nIn[7]:\\n# define compute_grad\\ndef compute_grad(theta, input_layer_size, hidden_layer_size, classes, X, y, lamb_da, epsilon=1e-4):\\n    n = np.size(theta)\\n    gradaproxy = np.zeros(n)\\n    for i in range(n):\\n        theta_plus = np.copy(theta) # Important for in numpy assign is shalow copy\\n        theta_plus[i] = theta_plus[i] + epsilon\\n        theta_minus = np.copy(theta)\\n        theta_minus[i] = theta_minus[i] - epsilon\\n        gradaproxy[i] = (costfunction(theta_plus, input_layer_size, hidden_layer_size, classes, X, y, lamb_da) - costfunction(theta_minus, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)) / (2.0 * epsilon)\\n    return gradaproxy\\n```\\n\\n---\\n\\n# 把一切放到一起\\n\\n先定义训练函数如下：\\n\\n```python\\nIn[8]:\\nfrom scipy import optimize\\n\\ndef train(input_layer_size, hidden_layer_size, classes, X, y, lamb_da):\\n    Theta_1, Theta_2 = random_init(input_layer_size, hidden_layer_size, classes, init_epsilon=0.12)\\n    nn_param = np.concatenate((Theta_1.ravel(), Theta_2.ravel()))\\n    final_nn = optimize.fmin_cg(costfunction, \\n                                np.concatenate((Theta_1.ravel(), Theta_2.ravel()), axis=0), \\n                                fprime=Gradient, \\n                                args=(input_layer_size,\\n                                      hidden_layer_size, \\n                                      classes, \\n                                      X, \\n                                      y,\\n                                      lamb_da))\\n    return final_nn\\n```\\n\\n---\\n\\n## 使用训练集进行训练\\n\\n我选取鸢尾花数据集的100条作为训练集，剩下50条作为测试集。\\n\\n```python\\nIn[9]:\\nX = iris.data[:100]\\ny = iris.target[:100]\\nlamb_da = 1.0 # must be float\\ninput_layer_size = 4\\nhidden_layer_size = 6\\nclasses = 3\\n\\nfinal_nn = train(input_layer_size, hidden_layer_size, classes, X, y, lamb_da)\\n\\n    Warning: Desired error not necessarily achieved due to precision loss.\\n             Current function value: 0.878999\\n             Iterations: 48\\n             Function evaluations: 152\\n             Gradient evaluations: 140\\n```\\n\\n---\\n\\n## 检测两种方式计算的梯度是否近似\\n\\n```python\\nIn[10]:\\n# gradient checking\\ngrad_aprox = compute_grad(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)\\ngrad_bp = Gradient(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)\\nprint (grad_aprox - grad_bp) < 1e-1\\n\\n    [ True  True  True  True  True  True  True  True  True  True  True  True\\n      True  True  True  True  True  True  True  True  True  True  True  True\\n      True  True  True  True  True  True  True  True  True  True  True  True\\n      True  True  True  True  True  True  True  True  True  True  True  True\\n      True  True  True]\\n```\\n\\n---\\n\\n## 对测试集使用训练得来的参数\\n\\n可以看到，测试集中50个样本有\\\\_个判定错误，其它\\\\_个分类正确。\\n\\n```python\\nIn[11]:\\ndef test(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da):\\n    Theta_1 = final_nn[:hidden_layer_size * (input_layer_size + 1)].reshape((hidden_layer_size, input_layer_size + 1))\\n    Theta_2 = final_nn[hidden_layer_size * (input_layer_size + 1):].reshape((classes, hidden_layer_size + 1))\\n    nsuccess = np.sum(np.argmax(predict(Theta_1, Theta_2, X), axis=1) == y)\\n    return nsuccess\\n\\nn = test(final_nn, input_layer_size, hidden_layer_size, classes, iris.data[100:], iris.target[100:], lamb_da)\\nprint n\\nn = test(final_nn, input_layer_size, hidden_layer_size, classes, iris.data[:100], iris.target[:100], lamb_da)\\nprint n\\n\\nOut[11]:\\n    47\\n    99\\n```\\n\\n---\\n\\n# 另一个例子：手写数字数据集\\n\\n最后是对同样著名的手写数字数据集的实验。\\n\\n### 载入并观察数据集：\\n\\n```python\\nIn[12]:\\ndigits = datasets.load_digits()\\n\\n# random it\\nperm = np.random.permutation(digits.target.size)\\ndigits.data = digits.data[perm]\\ndigits.target = digits.target[perm]\\nprint digits.data.shape\\nprint digits.target.shape\\nprint np.unique(digits.target)\\n\\nOut[12]:\\n    (1797, 64)\\n    (1797,)\\n    [0 1 2 3 4 5 6 7 8 9]\\n```\\n\\n---\\n\\n## 选择神经网络参数\\n\\n取1000个样本作为训练集，剩下作为测试集。\\n\\n```python\\nIn[13]:\\nX = digits.data[:1000]\\ny = digits.target[:1000]\\nlamb_da = 1.0 # must be float\\ninput_layer_size = 64\\nhidden_layer_size = 10\\nclasses = 10\\n\\nfinal_nn = train(input_layer_size, hidden_layer_size, classes, X, y, lamb_da)\\n\\nOut[13]:\\n    Warning: Desired error not necessarily achieved due to precision loss.\\n             Current function value: 0.594474\\n             Iterations: 965\\n             Function evaluations: 2210\\n             Gradient evaluations: 2189\\n```\\n\\n---\\n\\n## 进行梯度检测\\n\\n```python\\nIn[14]:\\n# gradient checking\\ngrad_aprox = compute_grad(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)\\ngrad_bp = Gradient(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)\\nprint np.all((grad_aprox - grad_bp) < 1e-1)\\n\\nOut[14]:\\n    True\\n```\\n\\n---\\n\\n## 对测试集使用训练得来的参数\\n\\n```python\\nIn[15]:\\n\\nn = test(final_nn, input_layer_size, hidden_layer_size, classes, digits.data[1000:], digits.target[1000:], lamb_da)\\nprint n\\n\\nn = test(final_nn, input_layer_size, hidden_layer_size, classes, digits.data[:1000], digits.target[:1000], lamb_da)\\nprint n\\n\\nOut[15]:\\n\\n    722\\n    991\\n```\\n\\n---\\n\\n<center>\\n<h3>BY REVERLAND</h3>\\n</center>\\n\\n\\n\"\n\n/***/ }\n\n});\n\n\n/** WEBPACK FOOTER **\n ** static/js/49.95dd52c9bf4904e9f0a2.js\n **/","module.exports = \"\\n\\n# 为何实现一个BP神经网络？\\n\\n> \\\"What I cannot create, I do not understand\\\"\\n> \\n> --- Richard Feynman,  February 1988\\n\\n---\\n\\n## 实现一个BP神经网络的7个步骤\\n\\n1. 选择神经网络 *结构*\\n2. *随机* 初始化权重\\n3. 实现 *前向传播* \\n4. 实现 *成本函数* $J(\\\\Theta)$\\n5. 实现反向传播算法并计算 *偏微分* $\\\\frac{\\\\partial}{\\\\partial\\n\\\\Theta_{jk}^{(i)}}J(\\\\Theta)$\\n6. 使用 *梯度检查* 并在检查后关闭\\n7. 使用梯度下降或其它优化算法和反向传播来 *优化* $\\\\Theta$ 的函数 $J(\\\\Theta)$\\n\\n---\\n\\n## 数据集\\n\\n我们选取著名的鸢尾花数据集作为神经网络作用的对象，首先先观察下数据。\\n\\n```python\\nIn[2]:\\nimport numpy as np\\nfrom sklearn import datasets\\niris = datasets.load_iris()\\n# random it\\nperm = np.random.permutation(iris.target.size)\\niris.data = iris.data[perm]\\niris.target = iris.target[perm]\\nprint iris.data.shape\\nprint iris.target.shape \\nnp.unique(iris.target)\\n#print iris\\nOut[2]:\\n    (150, 4)\\n    (150,)\\n\\n    array([0, 1, 2])\\n```\\n\\n可见，鸢尾花数据集有四个特征，分为三种类别，总共150个条数据。\\n\\n---\\n\\n# 选择人工神经网络结构\\n\\n![神经网络架构图](/images/nn.png)\\n\\n---\\n\\n# 随机初始化权重\\n\\n## 网络权重随机初始化\\n\\n 初始化 $\\\\Theta_{ij}^{(l)}$ 为  $[-\\\\epsilon, \\\\epsilon]$ 之间的随机值。\\n\\n 初始化函数可定义如下：\\n\\n```python\\nIn[3]:\\ndef random_init(input_layer_size, hidden_layer_size, classes, init_epsilon=0.12):\\n    Theta_1 = np.random.rand(hidden_layer_size, input_layer_size + 1) * 2 * init_epsilon -init_epsilon\\n    Theta_2 = np.random.rand(classes, hidden_layer_size + 1) * 2 * init_epsilon -init_epsilon\\n    return (Theta_1, Theta_2)\\n```\\n\\n---\\n\\n# 实现前向传播\\n\\n每一层都以此为公式计算下一层：\\n\\n$$\\nG_{\\\\theta^{(l)}(X)} = \\\\frac{1}{1 + e^{-X(\\\\Theta^{(l)})^T}}\\n$$\\n\\n注意添加偏差项。\\n\\n---\\n\\n### 前向传播函数定义如下\\n\\n```python\\nIn[4]:\\n# 先定义一个sigmoid函数\\ndef g(theta, X):\\n    return 1 / (1 + np.e ** (-np.dot(X, np.transpose(theta))))\\n# 前向传播函数\\ndef predict(Theta_1, Theta_2, X):\\n    h = g(Theta_1, np.hstack((np.ones((X.shape[0], 1)), X)))\\n    o = g(Theta_2, np.hstack((np.ones((h.shape[0], 1)), h)))\\n    return o\\n```\\n\\n---\\n\\n# 实现成本函数\\n\\n## 成本函数的数学表示\\n\\n$$\\nJ(\\\\Theta) = -\\\\frac{1}{m}[\\\\sum_{i=1}^m\\n\\\\sum_{k=1}^Ky_k^{(i)}\\\\log(h_\\\\Theta(x^{(i)}))_k + (1 -\\ny_k^{(i)})\\\\log(1-h_\\\\Theta(x^{(i)}))_k)] + \\\\frac{\\\\lambda}{2m}\\\\sum_{l=1}^{L-1}\\\\sum\\n_{i=1}^{s_l}\\\\sum_{j=1}^{s_{l+1}}(\\\\Theta_{ji}^{(l)})^2\\n$$\\n\\n$K$是输出层单元个数, $L$是层数，我们这里是三， $s_l$ 是层 $l$ 中单元数。\\n\\n---\\n\\n```python\\nIn[5]:\\ndef costfunction(nn_param, input_layer_size, hidden_layer_size, classes, X, y, lamb_da):\\n    Theta_1 = nn_param[:hidden_layer_size * (input_layer_size + 1)].reshape((hidden_layer_size, (input_layer_size + 1)))\\n    Theta_2 = nn_param[hidden_layer_size * (input_layer_size + 1):].reshape((classes, (hidden_layer_size + 1)))\\n    m = X.shape[0]\\n    # recode y\\n    y_recoded = np.eye(classes)[y,:]\\n\\n    # print y_recoded\\n    # calculate regularator\\n    regularator = (np.sum(Theta_1**2) + np.sum(Theta_2 ** 2)) * (lamb_da/(2*m))\\n\\n    a3 = predict(Theta_1, Theta_2, X)\\n\\n    J = 1.0 / m * np.sum(-1 * y_recoded * np.log(a3)-(1-y_recoded) * np.log(1-a3)) + regularator\\n    # print J\\n    return J\\n```\\n\\n因为scipy中优化函数要求输入参数是向量而不是矩阵，因此我们的函数实现也必须能够随时展开和复原矩阵。\\n\\n---\\n\\n# 实现反向传播\\n\\n* 正向传播计算网络输出\\n* 反向计算每一层的误差$\\\\sigma^{(l)}$\\n* 由误差累加计算得到成本函数的偏微分$\\\\frac{\\\\partial}{\\\\partial\\\\Theta_{ij}^{(l)}}J(\\\\Theta)$\\n\\n同样，在实现函数时因为要传递给`scipy.optimize`模块中的优化函数，必须能展开矩阵参数为向量并可随时复原。\\n\\n详细实现过程参见Andrew NG在Coursera上的讲座。\\n\\n数学推导过程请 *自行* 上coursera ml课程论坛搜索。\\n\\n---\\n\\n```python\\nIn[6]:\\ndef Gradient(nn_param, input_layer_size, hidden_layer_size, classes, X, y, lamb_da):\\n    \\n    Theta_1 = nn_param[:hidden_layer_size * (input_layer_size + 1)].reshape((hidden_layer_size, (input_layer_size + 1)))\\n    Theta_2 = nn_param[hidden_layer_size * (input_layer_size + 1):].reshape((classes, (hidden_layer_size + 1)))\\n    \\n    m = X.shape[0]\\n    \\n    Theta1_grad = np.zeros(Theta_1.shape);\\n    Theta2_grad = np.zeros(Theta_2.shape);\\n\\n    for t in range(m):\\n        # For the input layer, where l=1:\\n        # add a bias unit and forward propagate\\n        a1 = np.hstack((np.ones((X[t:t+1,:].shape[0], 1)), X[t:t+1,:]))\\n\\n        # For the hidden layers, where l=2:\\n        a2 = g(Theta_1, a1)\\n        a2 = np.hstack((np.ones((a2.shape[0], 1)), a2))\\n        a3 = g(Theta_2, a2)\\n\\n        yy = np.zeros((1, classes))\\n        yy[0][y[t]] = 1\\n        # For the delta values:\\n        delta_3 = a3 - yy;\\n```\\n\\n---\\n\\n续上页\\n\\n```python\\n        delta_2 = delta_3.dot(Theta_2) * (a2 * (1 - a2))\\n        delta_2 = delta_2[0][1:] # Taking of the bias row\\n        delta_2 = np.transpose(delta_2)\\n        delta_2 = delta_2.reshape(1, (np.size(delta_2)))\\n    \\n        # delta_1 is not calculated because we do not associate error with the input\\n\\n        # Big delta update\\n        Theta1_grad = Theta1_grad + np.transpose(delta_2).dot(a1);\\n        Theta2_grad = Theta2_grad + np.transpose(delta_3).dot(a2);\\n    \\n    Theta1_grad = 1.0 / m * Theta1_grad + lamb_da / m * np.hstack((np.zeros((Theta_1.shape[0], 1)), Theta_1[:, 1:]))\\n    Theta2_grad = 1.0 / m * Theta2_grad + lamb_da / m * np.hstack((np.zeros((Theta_2.shape[0], 1)), Theta_2[:, 1:]))\\n\\n    Grad = np.concatenate((Theta1_grad.ravel(), Theta2_grad.ravel()), axis=0)\\n    return Grad\\n```\\n\\n---\\n\\n# 梯度检测\\n\\n> \\\"But back prop as an algorithm has a lot of details and,you know, can be a\\nlittle bit tricky to implement.\\\"\\n> \\n> -- Andrew NG如是说\\n\\n- 即使看上去成本函数在一直减小，可能神经网络还是有问题。\\n- 梯度检测能让你确认，哦，我的实现还正常。\\n\\n---\\n\\n## 梯度检测原理\\n\\n简单的微分近似。但相比反向传播神经网络算法计算量更大。\\n\\n$$\\n\\\\frac{\\\\partial}{\\\\partial\\\\theta_{i}}J(\\\\theta) \\\\approx\\n\\\\frac{J(\\\\theta_{i}+\\\\epsilon) - J(\\\\theta_{i}+\\\\epsilon) }{2\\\\epsilon}\\n$$\\n\\n### 这种梯度计算实现如下：\\n\\n```python\\nIn[7]:\\n# define compute_grad\\ndef compute_grad(theta, input_layer_size, hidden_layer_size, classes, X, y, lamb_da, epsilon=1e-4):\\n    n = np.size(theta)\\n    gradaproxy = np.zeros(n)\\n    for i in range(n):\\n        theta_plus = np.copy(theta) # Important for in numpy assign is shalow copy\\n        theta_plus[i] = theta_plus[i] + epsilon\\n        theta_minus = np.copy(theta)\\n        theta_minus[i] = theta_minus[i] - epsilon\\n        gradaproxy[i] = (costfunction(theta_plus, input_layer_size, hidden_layer_size, classes, X, y, lamb_da) - costfunction(theta_minus, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)) / (2.0 * epsilon)\\n    return gradaproxy\\n```\\n\\n---\\n\\n# 把一切放到一起\\n\\n先定义训练函数如下：\\n\\n```python\\nIn[8]:\\nfrom scipy import optimize\\n\\ndef train(input_layer_size, hidden_layer_size, classes, X, y, lamb_da):\\n    Theta_1, Theta_2 = random_init(input_layer_size, hidden_layer_size, classes, init_epsilon=0.12)\\n    nn_param = np.concatenate((Theta_1.ravel(), Theta_2.ravel()))\\n    final_nn = optimize.fmin_cg(costfunction, \\n                                np.concatenate((Theta_1.ravel(), Theta_2.ravel()), axis=0), \\n                                fprime=Gradient, \\n                                args=(input_layer_size,\\n                                      hidden_layer_size, \\n                                      classes, \\n                                      X, \\n                                      y,\\n                                      lamb_da))\\n    return final_nn\\n```\\n\\n---\\n\\n## 使用训练集进行训练\\n\\n我选取鸢尾花数据集的100条作为训练集，剩下50条作为测试集。\\n\\n```python\\nIn[9]:\\nX = iris.data[:100]\\ny = iris.target[:100]\\nlamb_da = 1.0 # must be float\\ninput_layer_size = 4\\nhidden_layer_size = 6\\nclasses = 3\\n\\nfinal_nn = train(input_layer_size, hidden_layer_size, classes, X, y, lamb_da)\\n\\n    Warning: Desired error not necessarily achieved due to precision loss.\\n             Current function value: 0.878999\\n             Iterations: 48\\n             Function evaluations: 152\\n             Gradient evaluations: 140\\n```\\n\\n---\\n\\n## 检测两种方式计算的梯度是否近似\\n\\n```python\\nIn[10]:\\n# gradient checking\\ngrad_aprox = compute_grad(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)\\ngrad_bp = Gradient(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)\\nprint (grad_aprox - grad_bp) < 1e-1\\n\\n    [ True  True  True  True  True  True  True  True  True  True  True  True\\n      True  True  True  True  True  True  True  True  True  True  True  True\\n      True  True  True  True  True  True  True  True  True  True  True  True\\n      True  True  True  True  True  True  True  True  True  True  True  True\\n      True  True  True]\\n```\\n\\n---\\n\\n## 对测试集使用训练得来的参数\\n\\n可以看到，测试集中50个样本有\\\\_个判定错误，其它\\\\_个分类正确。\\n\\n```python\\nIn[11]:\\ndef test(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da):\\n    Theta_1 = final_nn[:hidden_layer_size * (input_layer_size + 1)].reshape((hidden_layer_size, input_layer_size + 1))\\n    Theta_2 = final_nn[hidden_layer_size * (input_layer_size + 1):].reshape((classes, hidden_layer_size + 1))\\n    nsuccess = np.sum(np.argmax(predict(Theta_1, Theta_2, X), axis=1) == y)\\n    return nsuccess\\n\\nn = test(final_nn, input_layer_size, hidden_layer_size, classes, iris.data[100:], iris.target[100:], lamb_da)\\nprint n\\nn = test(final_nn, input_layer_size, hidden_layer_size, classes, iris.data[:100], iris.target[:100], lamb_da)\\nprint n\\n\\nOut[11]:\\n    47\\n    99\\n```\\n\\n---\\n\\n# 另一个例子：手写数字数据集\\n\\n最后是对同样著名的手写数字数据集的实验。\\n\\n### 载入并观察数据集：\\n\\n```python\\nIn[12]:\\ndigits = datasets.load_digits()\\n\\n# random it\\nperm = np.random.permutation(digits.target.size)\\ndigits.data = digits.data[perm]\\ndigits.target = digits.target[perm]\\nprint digits.data.shape\\nprint digits.target.shape\\nprint np.unique(digits.target)\\n\\nOut[12]:\\n    (1797, 64)\\n    (1797,)\\n    [0 1 2 3 4 5 6 7 8 9]\\n```\\n\\n---\\n\\n## 选择神经网络参数\\n\\n取1000个样本作为训练集，剩下作为测试集。\\n\\n```python\\nIn[13]:\\nX = digits.data[:1000]\\ny = digits.target[:1000]\\nlamb_da = 1.0 # must be float\\ninput_layer_size = 64\\nhidden_layer_size = 10\\nclasses = 10\\n\\nfinal_nn = train(input_layer_size, hidden_layer_size, classes, X, y, lamb_da)\\n\\nOut[13]:\\n    Warning: Desired error not necessarily achieved due to precision loss.\\n             Current function value: 0.594474\\n             Iterations: 965\\n             Function evaluations: 2210\\n             Gradient evaluations: 2189\\n```\\n\\n---\\n\\n## 进行梯度检测\\n\\n```python\\nIn[14]:\\n# gradient checking\\ngrad_aprox = compute_grad(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)\\ngrad_bp = Gradient(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)\\nprint np.all((grad_aprox - grad_bp) < 1e-1)\\n\\nOut[14]:\\n    True\\n```\\n\\n---\\n\\n## 对测试集使用训练得来的参数\\n\\n```python\\nIn[15]:\\n\\nn = test(final_nn, input_layer_size, hidden_layer_size, classes, digits.data[1000:], digits.target[1000:], lamb_da)\\nprint n\\n\\nn = test(final_nn, input_layer_size, hidden_layer_size, classes, digits.data[:1000], digits.target[:1000], lamb_da)\\nprint n\\n\\nOut[15]:\\n\\n    722\\n    991\\n```\\n\\n---\\n\\n<center>\\n<h3>BY REVERLAND</h3>\\n</center>\\n\\n\\n\"\n\n\n/*****************\n ** WEBPACK FOOTER\n ** ./~/.npminstall/raw-loader/0.5.1/raw-loader!./loader/post-loader.js!./src/posts/2014-06-02-.md\n ** module id = 297\n ** module chunks = 49\n **/"],"sourceRoot":""}